{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760324983074\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "\n",
    "# read files into pandas dataframes\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "event_type = pd.read_csv('event_type.csv')\n",
    "log_feature = pd.read_csv('log_feature.csv')\n",
    "resource_type = pd.read_csv('resource_type.csv')\n",
    "severity_type = pd.read_csv('severity_type.csv')\n",
    "\n",
    "# create categorical variables out of locations\n",
    "train_crosstab = pd.merge(train, pd.crosstab(train.id, train.location), how='left', left_on='id', right_index=True)\n",
    "train_crosstab = train_crosstab.drop(['location'], axis=1).drop_duplicates()\n",
    "\n",
    "# create categorical variables out of resource types\n",
    "resource_type_crosstab = pd.merge(resource_type, pd.crosstab(resource_type.id, resource_type.resource_type), how='left', left_on='id', right_index=True)\n",
    "resource_type_crosstab = resource_type_crosstab.drop(['resource_type'], axis=1).drop_duplicates()\n",
    "\n",
    "# create categorical variables out of severity types\n",
    "severity_type_crosstab = pd.merge(severity_type, pd.crosstab(severity_type.id, severity_type.severity_type), how='left', left_on='id', right_index=True)\n",
    "severity_type_crosstab = severity_type_crosstab.drop(['severity_type'], axis=1).drop_duplicates()\n",
    "\n",
    "# create categorical variables out of event types\n",
    "event_type_crosstab = pd.merge(event_type, pd.crosstab(event_type.id, event_type.event_type), how='left', left_on='id', right_index=True)\n",
    "event_type_crosstab = event_type_crosstab.drop(['event_type'], axis=1).drop_duplicates()\n",
    "\n",
    "# pivot the log feature data to create a variable for each log feature; log feature volume is the scalar value \n",
    "log_feature_pivot = pd.merge(log_feature, log_feature.pivot(index='id', columns='log_feature', values='volume'), how='left', left_on='id', right_index=True)\n",
    "log_feature_pivot = log_feature_pivot.drop(['log_feature','volume'], axis=1).drop_duplicates()\n",
    "\n",
    "\n",
    "# merge dataframes into one\n",
    "merge1 = pd.merge(train_crosstab,log_feature_pivot,on='id',how='left')\n",
    "merge2 = pd.merge(merge1, event_type_crosstab,on='id',how='left')\n",
    "merge3 = pd.merge(merge2, resource_type_crosstab,on='id',how='left')\n",
    "merge4= pd.merge(merge3, severity_type_crosstab,on='id',how='left')\n",
    "\n",
    "\n",
    "\n",
    "#prepare the test file and put in the same format where the locations are categorical variables\n",
    "test_crosstab = pd.merge(test, pd.crosstab(test.id, test.location), how='left', left_on='id', right_index=True)\n",
    "test_crosstab = test_crosstab.drop(['location'], axis=1).drop_duplicates()\n",
    "\n",
    "\n",
    "# merge the test dataframes into one\n",
    "merge5 = pd.merge(test_crosstab,log_feature_pivot,on='id',how='left')\n",
    "merge6 = pd.merge(merge5, event_type_crosstab,on='id',how='left')\n",
    "merge7 = pd.merge(merge6, resource_type_crosstab,on='id',how='left')\n",
    "merge8= pd.merge(merge7, severity_type_crosstab,on='id',how='left')\n",
    "\n",
    "\n",
    "# concatenate the merged train and test dataframes to standardize columns across datasets\n",
    "concatenated_data = pd.concat([merge4,merge8],axis=0)\n",
    "\n",
    "# separate the train and test data back\n",
    "train_data = concatenated_data[pd.notnull(concatenated_data['fault_severity'])]\n",
    "test_data = concatenated_data[pd.isnull(concatenated_data['fault_severity'])]\n",
    "\n",
    "# replace NaNs with zeros\n",
    "train_data = train_data.fillna(value=0)\n",
    "test_data = test_data.fillna(value=0)\n",
    "\n",
    "# split the train dataset by columns into training columns and the target column\n",
    "\n",
    "cols = [col for col in train_data.columns if col not in ['id', 'fault_severity']]\n",
    "train_columns = train_data[cols]\n",
    "target_columns = train_data['fault_severity']\n",
    "\n",
    "train_columns.to_csv('train_columns.csv')\n",
    "\n",
    "# create Gradient Boosting Classifier object\n",
    "gbc = GradientBoostingClassifier(n_estimators=200, max_depth=7)\n",
    "\n",
    "# further split train dataset into train and test for cross-validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_columns, target_columns, test_size=0.2,random_state=3)\n",
    "\n",
    "# train the model\n",
    "gbc.fit(X_train, Y_train)\n",
    "\n",
    "# get predictions from the model\n",
    "Y_pred = gbc.predict(X_test)\n",
    "\n",
    "# show model accuracy\n",
    "print(metrics.accuracy_score(Y_test,Y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take only the attribute columns from the test dataset\n",
    "cols = [col for col in test_data.columns if col not in ['id', 'fault_severity']]\n",
    "test_columns = test_data[cols]\n",
    "\n",
    "# get predictions on the test dataset \n",
    "Y_pred_test = gbc.predict(test_columns)\n",
    "\n",
    "# add the predictions to the original file and select only id and Predictions columns\n",
    "predictions = pd.DataFrame(Y_pred_test, columns=['Predictions'])\n",
    "test_w_predictions = test_data.join(predictions)[['id', 'Predictions']]\n",
    "\n",
    "\n",
    "# format predictions according to the requested submission format\n",
    "test_w_predictions = test_w_predictions.pivot(index='id', columns='Predictions', values='Predictions')\n",
    "test_w_predictions = test_w_predictions.notnull().astype(int)\n",
    "test_w_predictions = test_w_predictions.rename(columns={0:'predict_0',1:'predict_1',2:'predict_2'})\n",
    "test_w_predictions.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
